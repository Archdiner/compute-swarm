"""
ComputeSwarm Job Templates
Pre-built templates for common ML/AI tasks
"""

from dataclasses import dataclass
from typing import Dict, Optional, List
from enum import Enum


class TemplateCategory(str, Enum):
    """Categories of job templates"""
    TRAINING = "training"
    INFERENCE = "inference"
    FINE_TUNING = "fine_tuning"
    DATA_PROCESSING = "data_processing"


@dataclass
class JobTemplate:
    """A job template definition"""
    name: str
    description: str
    category: TemplateCategory
    script: str
    default_requirements: Optional[str] = None
    default_timeout: int = 3600
    gpu_required: bool = True
    parameters: Optional[Dict[str, str]] = None  # {param_name: description}


# Template registry
_templates: Dict[str, JobTemplate] = {}


def register_template(template: JobTemplate):
    """Register a template in the global registry"""
    _templates[template.name] = template


def get_template(name: str) -> Optional[JobTemplate]:
    """Get a template by name"""
    return _templates.get(name)


def list_templates(category: Optional[TemplateCategory] = None) -> List[JobTemplate]:
    """List all templates, optionally filtered by category"""
    templates = list(_templates.values())
    if category:
        templates = [t for t in templates if t.category == category]
    return templates


def render_template(name: str, **params) -> str:
    """
    Render a template with the given parameters
    
    Args:
        name: Template name
        **params: Parameters to substitute in the template
        
    Returns:
        Rendered script string
    """
    template = get_template(name)
    if not template:
        raise ValueError(f"Template '{name}' not found")
    
    script = template.script
    for key, value in params.items():
        placeholder = f"{{{{{key}}}}}"  # {{key}}
        script = script.replace(placeholder, str(value))
    
    return script


# ============================================================================
# Built-in Templates
# ============================================================================

# PyTorch Training Template
PYTORCH_TRAINING_TEMPLATE = JobTemplate(
    name="pytorch_train",
    description="Train a PyTorch model with customizable architecture",
    category=TemplateCategory.TRAINING,
    gpu_required=True,
    default_timeout=7200,
    parameters={
        "model_class": "Model class name (e.g., resnet50)",
        "epochs": "Number of training epochs",
        "batch_size": "Training batch size",
        "learning_rate": "Learning rate",
    },
    script='''"""
PyTorch Training Job
Generated by ComputeSwarm
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Configuration
MODEL_CLASS = "{{model_class}}"
EPOCHS = {{epochs}}
BATCH_SIZE = {{batch_size}}
LEARNING_RATE = {{learning_rate}}

# Detect device
device = "cuda" if torch.cuda.is_available() else "mps" if hasattr(torch.backends, "mps") and torch.backends.mps.is_available() else "cpu"
print(f"Training on device: {device}")

# Simple demo model (replace with your model)
class SimpleModel(nn.Module):
    def __init__(self, input_size=784, hidden_size=256, num_classes=10):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        return self.fc2(x)

# Create model
model = SimpleModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Create synthetic dataset (replace with your data loading)
X = torch.randn(1000, 1, 28, 28)
y = torch.randint(0, 10, (1000,))
dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# Training loop
print(f"Starting training for {EPOCHS} epochs...")
for epoch in range(EPOCHS):
    total_loss = 0
    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(dataloader)
    print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}")

print("Training completed!")

# Save model
torch.save(model.state_dict(), "/tmp/model.pt")
print("Model saved to /tmp/model.pt")
'''
)
register_template(PYTORCH_TRAINING_TEMPLATE)


# HuggingFace Inference Template
HUGGINGFACE_INFERENCE_TEMPLATE = JobTemplate(
    name="huggingface_inference",
    description="Run inference with a HuggingFace model",
    category=TemplateCategory.INFERENCE,
    gpu_required=True,
    default_timeout=1800,
    default_requirements="transformers>=4.35.0\naccelerate>=0.24.0",
    parameters={
        "model_name": "HuggingFace model name (e.g., gpt2, facebook/opt-1.3b)",
        "prompt": "Input prompt for generation",
        "max_tokens": "Maximum tokens to generate",
    },
    script='''"""
HuggingFace Inference Job
Generated by ComputeSwarm
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Configuration
MODEL_NAME = "{{model_name}}"
PROMPT = """{{prompt}}"""
MAX_TOKENS = {{max_tokens}}

# Detect device
device = "cuda" if torch.cuda.is_available() else "mps" if hasattr(torch.backends, "mps") and torch.backends.mps.is_available() else "cpu"
print(f"Running on device: {device}")

# Load model and tokenizer
print(f"Loading model: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None
)

if device != "cuda":
    model = model.to(device)

# Tokenize input
inputs = tokenizer(PROMPT, return_tensors="pt").to(device)

# Generate
print("Generating response...")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=MAX_TOKENS,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )

# Decode and print
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\\n" + "="*50)
print("GENERATED TEXT:")
print("="*50)
print(response)
print("="*50)
'''
)
register_template(HUGGINGFACE_INFERENCE_TEMPLATE)


# LoRA Fine-tuning Template
LORA_FINETUNE_TEMPLATE = JobTemplate(
    name="lora_finetune",
    description="Fine-tune a model using LoRA (Low-Rank Adaptation)",
    category=TemplateCategory.FINE_TUNING,
    gpu_required=True,
    default_timeout=14400,  # 4 hours
    default_requirements="transformers>=4.35.0\npeft>=0.6.0\naccelerate>=0.24.0\ndatasets>=2.14.0",
    parameters={
        "base_model": "Base model to fine-tune (e.g., meta-llama/Llama-2-7b-hf)",
        "dataset_name": "HuggingFace dataset name",
        "epochs": "Number of training epochs",
        "lora_r": "LoRA rank (e.g., 8, 16, 32)",
    },
    script='''"""
LoRA Fine-tuning Job
Generated by ComputeSwarm
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

# Configuration
BASE_MODEL = "{{base_model}}"
DATASET_NAME = "{{dataset_name}}"
EPOCHS = {{epochs}}
LORA_R = {{lora_r}}

# Detect device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Fine-tuning on device: {device}")

if device != "cuda":
    print("WARNING: LoRA fine-tuning is much slower without GPU")

# Load model
print(f"Loading base model: {BASE_MODEL}")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None
)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=LORA_R,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
)

# Apply LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Load dataset
print(f"Loading dataset: {DATASET_NAME}")
dataset = load_dataset(DATASET_NAME, split="train[:1000]")  # Limit for demo

# Simple tokenization
def tokenize(example):
    return tokenizer(
        example["text"] if "text" in example else str(example),
        truncation=True,
        max_length=512,
        padding="max_length"
    )

tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)

print(f"Starting LoRA fine-tuning for {EPOCHS} epochs...")
print("(This is a demo - implement full training loop for production)")

# Save adapter
model.save_pretrained("/tmp/lora_adapter")
print("LoRA adapter saved to /tmp/lora_adapter")
'''
)
register_template(LORA_FINETUNE_TEMPLATE)


# Image Classification Template
IMAGE_CLASSIFICATION_TEMPLATE = JobTemplate(
    name="image_classification",
    description="Image classification with pretrained models",
    category=TemplateCategory.INFERENCE,
    gpu_required=True,
    default_timeout=600,
    parameters={
        "model_name": "Model name (resnet50, vit_b_16, efficientnet_b0)",
        "image_url": "URL of image to classify",
    },
    script='''"""
Image Classification Job
Generated by ComputeSwarm
"""

import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import urllib.request
import json

# Configuration
MODEL_NAME = "{{model_name}}"
IMAGE_URL = "{{image_url}}"

# Detect device
device = "cuda" if torch.cuda.is_available() else "mps" if hasattr(torch.backends, "mps") and torch.backends.mps.is_available() else "cpu"
print(f"Running on device: {device}")

# Load model
print(f"Loading model: {MODEL_NAME}")
if MODEL_NAME == "resnet50":
    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
elif MODEL_NAME == "vit_b_16":
    model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)
elif MODEL_NAME == "efficientnet_b0":
    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)
else:
    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)

model = model.to(device).eval()

# Image preprocessing
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Download image
print(f"Downloading image: {IMAGE_URL}")
urllib.request.urlretrieve(IMAGE_URL, "/tmp/input_image.jpg")
image = Image.open("/tmp/input_image.jpg").convert("RGB")

# Preprocess
input_tensor = transform(image).unsqueeze(0).to(device)

# Inference
print("Running inference...")
with torch.no_grad():
    outputs = model(input_tensor)
    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)

# Get top 5 predictions
top5_prob, top5_idx = torch.topk(probabilities, 5)

# Load ImageNet labels
LABELS_URL = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
urllib.request.urlretrieve(LABELS_URL, "/tmp/imagenet_classes.txt")
with open("/tmp/imagenet_classes.txt") as f:
    labels = [line.strip() for line in f.readlines()]

# Print results
print("\\n" + "="*50)
print("TOP 5 PREDICTIONS:")
print("="*50)
for i in range(5):
    print(f"{i+1}. {labels[top5_idx[i]]}: {top5_prob[i].item()*100:.2f}%")
print("="*50)
'''
)
register_template(IMAGE_CLASSIFICATION_TEMPLATE)


# Benchmark Template
GPU_BENCHMARK_TEMPLATE = JobTemplate(
    name="gpu_benchmark",
    description="Benchmark GPU compute performance",
    category=TemplateCategory.DATA_PROCESSING,
    gpu_required=True,
    default_timeout=300,
    parameters={
        "matrix_size": "Size of matrices for benchmark (e.g., 4096)",
        "iterations": "Number of benchmark iterations",
    },
    script='''"""
GPU Benchmark Job
Generated by ComputeSwarm
"""

import torch
import time

# Configuration
MATRIX_SIZE = {{matrix_size}}
ITERATIONS = {{iterations}}

# Detect device
device = "cuda" if torch.cuda.is_available() else "mps" if hasattr(torch.backends, "mps") and torch.backends.mps.is_available() else "cpu"
print(f"Benchmarking on device: {device}")

# Get device info
if device == "cuda":
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

print(f"\\nMatrix size: {MATRIX_SIZE}x{MATRIX_SIZE}")
print(f"Iterations: {ITERATIONS}")

# Create matrices
a = torch.randn(MATRIX_SIZE, MATRIX_SIZE, device=device)
b = torch.randn(MATRIX_SIZE, MATRIX_SIZE, device=device)

# Warmup
for _ in range(5):
    c = torch.matmul(a, b)

if device == "cuda":
    torch.cuda.synchronize()

# Benchmark
print("\\nRunning benchmark...")
start = time.perf_counter()

for i in range(ITERATIONS):
    c = torch.matmul(a, b)
    if device == "cuda":
        torch.cuda.synchronize()

end = time.perf_counter()

# Calculate TFLOPS
ops_per_matmul = 2 * MATRIX_SIZE ** 3
total_ops = ops_per_matmul * ITERATIONS
elapsed = end - start
tflops = (total_ops / elapsed) / 1e12

# Results
print("\\n" + "="*50)
print("BENCHMARK RESULTS:")
print("="*50)
print(f"Total time: {elapsed:.3f} seconds")
print(f"Time per operation: {elapsed/ITERATIONS*1000:.2f} ms")
print(f"Performance: {tflops:.2f} TFLOPS")
print("="*50)
'''
)
register_template(GPU_BENCHMARK_TEMPLATE)


def get_template_help() -> str:
    """Get help text for all available templates"""
    lines = ["Available Job Templates:", "=" * 40, ""]
    
    for category in TemplateCategory:
        templates = list_templates(category)
        if templates:
            lines.append(f"\n{category.value.upper()}:")
            lines.append("-" * 30)
            for t in templates:
                lines.append(f"  {t.name}")
                lines.append(f"    {t.description}")
                if t.parameters:
                    lines.append("    Parameters:")
                    for param, desc in t.parameters.items():
                        lines.append(f"      --{param}: {desc}")
                lines.append("")
    
    return "\n".join(lines)

